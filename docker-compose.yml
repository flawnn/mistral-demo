version: "3.9"

services:
  # ML inference backend (FastAPI)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - ./backend/.env
    ports:
      - "8000:8000"
    volumes:
      - ./backend/data:/app/data
      - ./backend/processed_data:/app/processed_data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Next.js frontend + BFF
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - ./.env
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "3000:3000"
    restart: unless-stopped
